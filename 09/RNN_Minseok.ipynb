{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "RNN Minseok.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Spinkk/Implementing-ANNs-with-Tensorflow/blob/main/09/RNN_Minseok.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQRWnhmhT09Q",
        "outputId": "f9c4ebae-dc29-4635-e5c5-aeafc57422b4"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "%config Completer.use_jedi = False"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Config option `use_jedi` not recognized by `IPCompleter`.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceE5WpUeT09a"
      },
      "source": [
        "def data_gen_batch (max_t=10, batch_size=32):\n",
        "    \"\"\"\n",
        "    Generate a batch of (input, label) pairs. Input is two query and one sequence digits for each timepoint.\n",
        "    Output indicates which of two queries occur more frequently for each timepoint cumulatively.    \n",
        "    \"\"\"\n",
        "\n",
        "    while True:\n",
        "        t = np.random.randint(1, max_t+1) # fix time length of one sample\n",
        "        \n",
        "        x = np.random.randint(0, 10, (batch_size,t), dtype=np.int32) # actual input sequence\n",
        "        c1 = np.random.randint(0, 10, (batch_size,t), dtype=np.int32)  # context 1\n",
        "        c2 = np.random.randint(0, 10, (batch_size,t), dtype=np.int32)  # context 2\n",
        "\n",
        "        def cumulative_occurence (seq, context):\n",
        "            occ = np.zeros((batch_size,t))\n",
        "            # occurence for a slice of sequence until time point i\n",
        "            for i in range(t):\n",
        "                # context digit at time i is broadcasted to match dim of seq\n",
        "                cb = np.repeat(np.array([context[:,i]]).T, i+1, axis=1)\n",
        "                # count how often digits match for each sample in a batch\n",
        "                occ[:,i] = np.count_nonzero(seq[:, :i+1] == cb, axis=1)\n",
        "            return occ\n",
        "        \n",
        "        count1 = cumulative_occurence(x,c1)\n",
        "        count2 = cumulative_occurence(x,c2)\n",
        "        label = np.array([count1 >= count2, count2 >= count1])  # (1,0): c1>c2, (0,1):c2>c1, (1,1):c1=c2\n",
        "        label = np.transpose(label, (1,2,0))\n",
        "        input = tf.concat([tf.one_hot(x,10), tf.one_hot(c1,10), tf.one_hot(c2,10)],-1)\n",
        "        input = np.transpose(input, (1,0,2))\n",
        "              \n",
        "        yield (input,label)  # one-hot encoded then concatanated\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "ds = tf.data.Dataset.from_generator(generator=data_gen_batch,\n",
        "                                    output_types=(tf.int32, tf.int32),\n",
        "                                    # (batch,time,30), (2,batch,time,10)\n",
        "                                    output_shapes=((None,None,30), (None,None,2))\n",
        "                                    )"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAQn5dT4eQRR",
        "outputId": "e9798347-dd40-491a-eeb7-eddbf2d7125a"
      },
      "source": [
        "for input,label in ds.take(5):\n",
        "    print(label.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 5, 2)\n",
            "(32, 9, 2)\n",
            "(32, 10, 2)\n",
            "(32, 7, 2)\n",
            "(32, 1, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ltw2qRqfT09g"
      },
      "source": [
        "# Weight matrices, their meaning and required dimensionality\n",
        "\n",
        "X means element wise product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF_S3_AsT09l"
      },
      "source": [
        "![image.png](attachment:image.png)\n",
        "\n",
        "\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "![image-3.png](attachment:image-3.png)\n",
        "\n",
        "![image-4.png](attachment:image-4.png)\n",
        "\n",
        "![image-5.png](attachment:image-5.png)\n",
        "\n",
        "![image-6.png](attachment:image-6.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93tTbvNhT09p"
      },
      "source": [
        "class LSTM_cell (tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_dim = 100):\n",
        "        super(LSTM_cell, self).__init__()\n",
        "        self.h = hidden_dim\n",
        "\n",
        "\n",
        "    def build(self, input_shape=100):        \n",
        "        d = input_shape[1]  # This was the problem: input_shape is not a scalar\n",
        "        h = self.h\n",
        "        \n",
        "        # dimension of output vector and weight matrix\n",
        "        output_shape = (h,)  \n",
        "        weight_shape = (h+d,h) \n",
        "        \n",
        "        # forget gate\n",
        "        self.w_f = self.add_weight(shape= weight_shape,\n",
        "                                   initializer=tf.random_normal_initializer(),\n",
        "                                   trainable=True)\n",
        "        \n",
        "        self.b_f = self.add_weight(shape=output_shape,\n",
        "                                   # bias of forget gate is initially 1\n",
        "                                   initializer=tf.keras.initializers.Constant(value=1.0),\n",
        "                                   trainable=True)\n",
        "        # input gate\n",
        "        self.w_i = self.add_weight(shape=weight_shape,\n",
        "                                   initializer=tf.random_normal_initializer(),\n",
        "                                   trainable=True)\n",
        "        \n",
        "        self.b_i = self.add_weight(shape=output_shape,\n",
        "                                   initializer=tf.random_normal_initializer(),\n",
        "                                   trainable=True)\n",
        "        \n",
        "        # candidate layer\n",
        "        self.w_c = self.add_weight(shape=weight_shape,\n",
        "                                   initializer=tf.random_normal_initializer(),\n",
        "                                   trainable=True)\n",
        "        \n",
        "        self.b_c = self.add_weight(shape=output_shape, \n",
        "                                   initializer=tf.random_normal_initializer(),\n",
        "                                   trainable=True)\n",
        "        \n",
        "        # output gate\n",
        "        self.w_o = self.add_weight(shape=weight_shape,\n",
        "                                   initializer=tf.random_normal_initializer(),\n",
        "                                   trainable=True)\n",
        "        \n",
        "        self.b_o = self.add_weight(shape=output_shape, \n",
        "                                   initializer=tf.random_normal_initializer(),\n",
        "                                   trainable=True)\n",
        "        \n",
        "\n",
        "    def call(self, x_t, h_t, C_t):\n",
        "        print('x,h,c', x_t.shape, h_t.shape, C_t.shape)\n",
        "        # concatenate previous hidden_state and current input (shape is 20,1)\n",
        "        concat = tf.concat([h_t, x_t], axis=1) # across axis with size 30 for input!\n",
        "        print('concat shape', concat.shape)\n",
        "        # calculate f_t (the forget gate)\n",
        "        f_t = tf.nn.sigmoid( tf.linalg.matmul(concat,self.w_f) + self.b_f )   # weight shapes must be 20 in second dimension, \n",
        "                                                                               # otherwise matmul does not work\n",
        "        # calculate the input gate i_t\n",
        "        i_t = tf.nn.sigmoid( tf.linalg.matmul(concat,self.w_i) + self.b_i )\n",
        "        \n",
        "        # calculate the Cell state candidates (C_hat)\n",
        "        C_hat_t = tf.nn.tanh( tf.linalg.matmul(concat,self.w_c) + self.b_c )\n",
        "        \n",
        "        # calculate the Cell state (C_t)\n",
        "        C_t = f_t * C_t + i_t * C_hat_t\n",
        "        \n",
        "        # calculate the output state (o_t)\n",
        "        o_t = tf.nn.sigmoid(tf.linalg.matmul(concat,self.w_o) + self.b_o )\n",
        "        \n",
        "        # calculate the new hidden state (h_t)\n",
        "        h_t = o_t * tf.nn.tanh(C_t)\n",
        "        \n",
        "        return h_t, C_t"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj1rASg0YEnU",
        "outputId": "f64a6143-0e46-4efb-f7ab-d95b3c0e553a"
      },
      "source": [
        "cell = LSTM_cell(hidden_dim=4)\n",
        "cell(tf.constant([[1.0, 2, 3]]), tf.constant([[1, 1, 1.0, 1]]), tf.constant([[0.0]]))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x,h,c (1, 3) (1, 4) (1, 1)\n",
            "concat shape (1, 7)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
              " array([[ 0.03137889, -0.01149101, -0.09742334,  0.0543483 ]],\n",
              "       dtype=float32)>, <tf.Tensor: shape=(1, 4), dtype=float32, numpy=\n",
              " array([[ 0.06732637, -0.02697579, -0.1783053 ,  0.09609307]],\n",
              "       dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zna3KvZVT091"
      },
      "source": [
        "class LSTM_net (tf.keras.Model):\n",
        "    '''\n",
        "    Build a LSTM net with a single recurrent node\n",
        "    '''\n",
        "    def __init__(self, hidden_dim=100, readin_dim=1000, batch_size=32):\n",
        "        super(LSTM_net, self).__init__()\n",
        "        self.h = hidden_dim\n",
        "        self.readin = tf.keras.layers.Dense(readin_dim)\n",
        "        self.recurrent = LSTM_cell(hidden_dim = self.h)\n",
        "        self.readout = tf.keras.layers.Dense(2, activation = \"sigmoid\")\n",
        "        \n",
        "        # h_t and C_t of LSTM cell is saved externally\n",
        "        # (batch_size, h) to have different states for each sample\n",
        "        self.hidden_state = tf.Variable(tf.zeros((batch_size,self.h)), trainable=False) \n",
        "        self.cell_state = tf.Variable(tf.ones((batch_size,self.h)), trainable = False)\n",
        "        \n",
        "        \n",
        "    @tf.function\n",
        "    # TODO: for loop doesn't work with tf.function\n",
        "    def call(self, x):            \n",
        "\n",
        "        # GRAPH MODE: doesn't work as quite as expected?\n",
        "        init_state = (0,  # time counter\n",
        "                      tf.zeros((x.shape[1],self.h)),  # h(0)\n",
        "                      tf.ones((x.shape[1],self.h)),  # c(0)\n",
        "                      tf.TensorArray(dtype = tf.float32, size=0, dynamic_size=True))  # res array\n",
        "        condition = lambda i,*_: i < x.shape[0]  # iterate over all time slice\n",
        "        \n",
        "        def body (i, h_t, c_t, ta):\n",
        "            # forward computation\n",
        "            digit = x[i]\n",
        "            digit = self.readin(digit)\n",
        "            h_t, c_t = self.recurrent(digit, h_t, c_t)\n",
        "            digit = self.readout(h_t)\n",
        "            ta = ta.write(i,digit)  # save results\n",
        "            return i+1, h_t, c_t, ta  # counter+1\n",
        "\n",
        "        _ = tf.while_loop(condition, body, init_state)\n",
        "        return self.outputs\n",
        "\n",
        "\n",
        "        # # EAGER TENSOR MODE\n",
        "        # # reset hidden and cell state to an inital state\n",
        "        # self.hidden_state.assign(tf.zeros((x.shape[1],self.h)))\n",
        "        # self.cell_state.assign(tf.ones((x.shape[1],self.h)))\n",
        "        # # array to save output sequences of the network\n",
        "        # self.outputs = tf.TensorArray(dtype = tf.float32, size=0, dynamic_size=True)\n",
        "        # # unrolling network by iterative forward computation\n",
        "        # # x has dimension of (time, batch, 30)\n",
        "        # for i,digit in enumerate(x):  \n",
        "        #     digit = self.readin(digit)\n",
        "        #     # update h_t and C_t to save externally\n",
        "        #     self.hidden_state, self.cell_state = self.recurrent(digit, self.hidden_state, self.cell_state)\n",
        "        #     digit = self.readout(self.hidden_state)\n",
        "        #     self.outputs = self.outputs.write(i, digit)  # save output for time i\n",
        "        # return self.outputs"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYB0CNX0T095",
        "outputId": "58e07dcf-69a7-4122-da94-9d2654383028"
      },
      "source": [
        "lstm = LSTM_net()\n",
        "for x,target in ds.take(1):\n",
        "    res = lstm(x)\n",
        "    print(res)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x,h,c (32, 1000) (32, 100) (32, 100)\n",
            "concat shape (32, 1100)\n",
            "x,h,c (32, 1000) (32, 100) (32, 100)\n",
            "concat shape (32, 1100)\n",
            "tf.Tensor(<unprintable>, shape=(), dtype=variant)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}