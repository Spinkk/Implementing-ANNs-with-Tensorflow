{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGQfCB5gmm3D"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tpMQ3A11luLM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# select plot style\n",
    "plt.style.use('fivethirtyeight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftAadfJJn0dP"
   },
   "source": [
    "## Generator\n",
    "The Generator yields<br>\n",
    "a sequence<br>\n",
    "a query and<br>\n",
    "a label<br>\n",
    "\n",
    "The label indicates which element is more common in the sequence<br>\n",
    "[ [ 1 ] [ 0 ] ] first more common<br>\n",
    "[ [ 0 ] [ 1 ] ] first more common<br>\n",
    "[ [ 1 ] [ 1 ] ] equally common<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rFujAcPh_-uS"
   },
   "outputs": [],
   "source": [
    "def calc_label(seq, query):\n",
    "    n1 = tf.reduce_sum(tf.cast(tf.equal(seq, query[0]), tf.int32)) # count frequency of first element\n",
    "    n2 = tf.reduce_sum(tf.cast(tf.equal(seq, query[1]), tf.int32)) # count frequency of second element \n",
    "    \n",
    "    return tf.constant([int(n1>=n2), int(n2>=n1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mEUxhcoIvn0d"
   },
   "outputs": [],
   "source": [
    "seq_len = 10\n",
    "\n",
    "# a generator that yields sequences of length seq_len\n",
    "# and queries\n",
    "def sequence_and_query():\n",
    "  while True:\n",
    "      seq = tf.random.uniform(shape=(seq_len, 1), minval=0, maxval=10, dtype=tf.int32)\n",
    "      query = tf.random.uniform(shape=(2,1), minval=0, maxval=10, dtype=tf.int32)\n",
    "      label = calc_label(seq, query)\n",
    "      yield seq, query, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myJ1MYJmxxQ-"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "V8DeUjrewDyh"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "from_generator() got an unexpected keyword argument 'output_signature'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-fd021e4816c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m dataset_gen = tf.data.Dataset.from_generator(sequence_and_query,\n\u001b[0m\u001b[1;32m      2\u001b[0m                                              output_signature=(\n\u001b[1;32m      3\u001b[0m                                               \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                               \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                               tf.TensorSpec(shape=(2), dtype=tf.int32)))\n",
      "\u001b[0;31mTypeError\u001b[0m: from_generator() got an unexpected keyword argument 'output_signature'"
     ]
    }
   ],
   "source": [
    "dataset_gen = tf.data.Dataset.from_generator(sequence_and_query,\n",
    "                                             output_signature=(\n",
    "                                              tf.TensorSpec(shape=(10,1), dtype=tf.int32),\n",
    "                                              tf.TensorSpec(shape=(2,1), dtype=tf.int32),\n",
    "                                              tf.TensorSpec(shape=(2), dtype=tf.int32)))\n",
    "\n",
    "\n",
    "# We can take a fixed amount of datapoints from our generator dataset\n",
    "num = 200\n",
    "data_train_num = num * 64\n",
    "data_test_num = num//10 * 64\n",
    "\n",
    "train_dataset = dataset_gen.take(data_train_num).cache()\n",
    "test_dataset = dataset_gen.take(data_test_num).cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGIsoIKKx9T3"
   },
   "source": [
    "## Inspect data\n",
    "\n",
    "we can access <br> the sequence with elem[0]<br> the query with elem[1] and\n",
    "<br> the label with elem[2]\n",
    "<nr>for elem in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8qsGGq_x8LO",
    "outputId": "a25708c9-01b1-41ae-92a4-67da1f4616a4"
   },
   "outputs": [],
   "source": [
    "for elem in train_dataset.take(1):\n",
    "  print(\"Sequence:\", \"\\n\", elem[0], \"\\n\")\n",
    "  print(\"Query:\", \"\\n\", elem[1], \"\\n\")\n",
    "  print(\"Label:\", \"\\n\", elem[2])\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ev9GhYphzqyG"
   },
   "source": [
    "## Data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMOKyngszC2-"
   },
   "outputs": [],
   "source": [
    "# create minibatches\n",
    "batch_size = 64\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "# encode data one-hot\n",
    "train_dataset = train_dataset.map(lambda seq, query, label: \n",
    "                                                (tf.one_hot(seq, 10), tf.one_hot(query, 10), label), \n",
    "                                      num_parallel_calls=tf.data.experimental.AUTOTUNE).cache()\n",
    "\n",
    "test_dataset = test_dataset.map(lambda seq, query, label: \n",
    "                                                (tf.one_hot(seq, 10), tf.one_hot(query, 10), label), \n",
    "                                      num_parallel_calls=tf.data.experimental.AUTOTUNE).cache()\n",
    "\n",
    "# Prefetch\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0-DrjjfFOWJ"
   },
   "source": [
    "# The Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAMN2wdmjkIG"
   },
   "source": [
    "## LSTM Cell\n",
    "\n",
    "Before we build the cell we should talk about input shapes and unit counts\n",
    "\n",
    "The **gates**, **cell state** and **hidden state** are multiplied pointwise which each other at some point, so they need to have the same length or unit count.<br>\n",
    "we will denote this with ***unit_num***\n",
    "\n",
    "The problem is that the gates get a concatenated input which means that their output will not be able to be multiplied pointwise with for example the cell state (shape error (batch_size, **6**, 1, 10) != (batch_size, ****3, 1, 10)\n",
    "\n",
    "So what we do, is set the units of the gates to **unit_num/2** and then reshape<br> the output (batch_size, 6, 1, 5) to (batch_size, 3, 1, 10) so that we match the cell state shape\n",
    "\n",
    "The **input of the gates** (and also the **candidate layer**) consists of the concatination of a hidden state and the current input<br>\n",
    "so we have an input shape of ***unit_num + 3 • 10***, because the current input consists of one sequence element and two query numbers which are all one-hot(10)  encoded\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eYSlvTDTFQM1"
   },
   "outputs": [],
   "source": [
    "class LSTM_Cell(Model): \n",
    "    \n",
    "    def __init__(self, unit_num = 10):\n",
    "        super(LSTM_Cell, self).__init__()\n",
    "        \n",
    "        # Our LSTM_Cell needs the following things\n",
    "\n",
    "        # Cell state\n",
    "        self.cell_state = tf.zeros(shape = (batch_size,3,1,unit_num), dtype = tf.int32)\n",
    "\n",
    "        # Hidden state\n",
    "        self.hidden_state = tf.zeros(shape = (batch_size,3,1,unit_num), dtype = tf.int32)\n",
    "\n",
    "        # A layer that proposes a new cell state candidate\n",
    "        # based on the input and the hidden state\n",
    "        self.candidate = tf.keras.layers.Dense(units=unit_num/2, \n",
    "                                               activation=\"tanh\",\n",
    "                                               #input_shape = (batch_size, unit_num + 30)\n",
    "                                               )\n",
    "\n",
    "        # The Gates\n",
    "        # all gates (their outputs) will be multiplied pointwise \n",
    "        # with the cell state, so they need as much units as the cell state has elements\n",
    "\n",
    "        # Forget Gate\n",
    "        # f = σ(Wf[ht−1,xt]+bf)\n",
    "        self.forget_gate = tf.keras.layers.Dense(units=unit_num/2, \n",
    "                                                 activation=\"sigmoid\",\n",
    "                                                 #input_shape = (batch_size, unit_num + 30),\n",
    "                                                 bias_initializer='ones'\n",
    "                                                 )\n",
    "        \n",
    "        # Input Gate\n",
    "        # i = σ(Wi[ht−1,xt]+bi)\n",
    "        self.input_gate = tf.keras.layers.Dense(units=unit_num/2, \n",
    "                                                 activation=\"sigmoid\",\n",
    "                                                 #input_shape = (batch_size, unit_num + 30)\n",
    "                                                 )\n",
    "        \n",
    "        # Output Gate\n",
    "        # o = σ(Wo[ht−1,xt]+b0)\n",
    "        self.output_gate = tf.keras.layers.Dense(units=unit_num/2, \n",
    "                                                 activation=\"sigmoid\",\n",
    "                                                 #input_shape = (batch_size, unit_num + 30)\n",
    "                                                 )\n",
    "        \n",
    "        # (64, 6, 1, 10) -> (64, 3, 1, 10) \n",
    "        self.reshape_layer = tf.keras.layers.Reshape(target_shape = (3,1,unit_num))\n",
    "        \n",
    "        \n",
    "    \n",
    "       \n",
    "    # LSTM cells are called with\n",
    "    # current input x of a single time step\n",
    "    # so one sequence element and the query\n",
    "    # and also with the hidden_state of the previous time step and the cell_state\n",
    "    # which are both vectors\n",
    "    @tf.function\n",
    "    def call(self, hidden_state, xt, cell_state):\n",
    "     \n",
    "        # first we concatenate the hidden state (ht-1) with the current input (xt)\n",
    "        # as they form the vector (v) that our cell works with as a new input\n",
    "        # [ht−1,xt] = v\n",
    "        cell_input = tf.concat([tf.cast(hidden_state, dtype=tf.int32), tf.cast(xt, dtype=tf.int32)], axis = 1) # SHAPE PROBLEM\n",
    "        print(cell_input)\n",
    "        # then we propose a new cell state, based on this combined input\n",
    "        cell_state_candidate = self.reshape_layer(self.candidate(cell_input))\n",
    "\n",
    "        # next we feed the cell input [ht−1,xt] through the different gates\n",
    "        forget_gate_out = self.reshape_layer(self.forget_gate(cell_input))\n",
    "        input_gate_out  = self.reshape_layer(self.input_gate(cell_input))\n",
    "        output_gate_out = self.reshape_layer(self.output_gate(cell_input))\n",
    "       \n",
    "        # with whatever came through the gates we update the cell state\n",
    "        #print(\"cell_state.shape: \", cell_state.shape)\n",
    "        cell_state =  (tf.math.multiply(tf.cast(forget_gate_out, dtype=tf.float32), tf.cast(cell_state, dtype=tf.float32)) + \n",
    "                           tf.math.multiply(tf.cast(input_gate_out, dtype=tf.float32), tf.cast(cell_state_candidate, dtype=tf.float32)))\n",
    "    \n",
    "\n",
    "\n",
    "        # the new cell state now determines together with the output gate\n",
    "        # the new hidden state\n",
    "        hidden_state = output_gate_out * cell_state\n",
    "\n",
    "       \n",
    "        # for the next time step we will need \n",
    "        # the previous hidden state and the previous cell state\n",
    "        return hidden_state, cell_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vToqQurrh8z6"
   },
   "outputs": [],
   "source": [
    "class LSTM(Model): \n",
    "    \n",
    "    def __init__(self, unit_num = 10):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.unit_num = unit_num\n",
    "\n",
    "        # The first thing we need is a LSTM cell\n",
    "        self.lstm_cell = LSTM_Cell(unit_num)\n",
    "\n",
    "        # Once we pushed our sequence through the LSTM cell\n",
    "        # we are presented a hidden state of shape unit_num\n",
    "        # this hidden state needs to be pushed through an output layer\n",
    "        # so that the network can model our labels which have a shape 2 \n",
    "        self.output_layer = tf.keras.layers.Dense(units=2, \n",
    "                                                  activation=\"sigmoid\",\n",
    "                                                  #input_shape = (batch_size, 3, 1, unit_num)\n",
    "                                                  )\n",
    "        \n",
    "        self.flatten_layer = tf.keras.layers.Flatten()\n",
    "       \n",
    "   \n",
    "    # The Model is called with a sequence (shape seq_len) and a query (shape 2)\n",
    "    @tf.function\n",
    "    def call(self, data):\n",
    "\n",
    "        seq = data[0]\n",
    "        query = data[1]\n",
    "\n",
    "   \n",
    "\n",
    "        # Cell state\n",
    "        cell_state = tf.zeros(shape = (batch_size,3,1, self.unit_num), dtype = tf.float32)\n",
    "\n",
    "   \n",
    "        # Hidden state\n",
    "        hidden_state = tf.zeros(shape = (batch_size,3,1, self.unit_num), dtype = tf.float32)\n",
    "\n",
    "  \n",
    "        # The length of the sequence determine for how many timesteps\n",
    "        # we activate our LSTM cell\n",
    "        # We iterate over the sequence length and \n",
    "        # select from all batch elements the respective number\n",
    "        # For example the 2nd sequence element from all 64 batch elements (bach size  = 64)\n",
    "        for i in tf.range(seq.shape[-1]):\n",
    "        \n",
    "            \n",
    "            # The sequence elements are concatinated with the query and\n",
    "            # passed through the LSTM cell\n",
    "            seq_i = seq[:,i,:]\n",
    "\n",
    "            # The input xt (batch_size, 3, 1, 10) for the LSTM cell is the \n",
    "            # concatination of one sequence element and the query \n",
    "            xt = tf.concat((tf.expand_dims(seq_i, axis = 1), query), axis = 1)\n",
    "            \n",
    "            # The LSTM cell also need a hidden state and a cell state as input\n",
    "            # for that we take current states of the cell which will \n",
    "            # act as the previous states during the call\n",
    "            # we catch the hidden state for the final output\n",
    "            #hidden_state, cell_state = self.lstm_cell(self.lstm_cell.hidden_state, xt, self.lstm_cell.cell_state)\n",
    "            hidden_state, cell_state = self.lstm_cell(hidden_state, xt, cell_state)\n",
    "\n",
    "\n",
    "            #i+= 1\n",
    "            \n",
    "\n",
    "        # Now that the sequence has passed through the LSTM cell we can \n",
    "        # push the last hidden state through the output layer\n",
    "        # and return the output\n",
    "        # and also flatten because right now the shape is \n",
    "        # (batch_size, 3, 1, 10) and we want (batch_size, 30)\n",
    "        # so that the output can be (batch_size, 2)\n",
    "        output = self.output_layer(self.flatten_layer(hidden_state))\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6Oe6lk12IWT"
   },
   "source": [
    "## Test LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bAEX3dcfnsV_",
    "outputId": "7490213b-da20-46c6-c690-dd767edbbc4a"
   },
   "outputs": [],
   "source": [
    "lstm_model = LSTM(unit_num = 10)\n",
    "\n",
    "for elem in train_dataset.take(1):\n",
    "    test_data = elem\n",
    "\n",
    "print(\"Output: \", lstm_model(test_data)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ujyAUa_Ec0xU"
   },
   "outputs": [],
   "source": [
    "for elem in train_dataset:\n",
    "  a = elem\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mzIXxg8Dc4yl",
    "outputId": "c037ab8a-caff-400a-abb3-15919cf9b6a2"
   },
   "outputs": [],
   "source": [
    "a[2][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksOIb678cQjV"
   },
   "source": [
    "## Define Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TiN95MGcSb9"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, data, loss_function, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model(data)\n",
    "        label = tf.cast(data[2], dtype=tf.float32)\n",
    "        loss = loss_function(pred, label)\n",
    "        \n",
    "        # if the distance between prediction and label is < 0.5 \n",
    "        # for the respective query elements we say the prediction is correct\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.greater(pred,0.5), tf.equal(label,1.0)), dtype=tf.float32))\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss, accuracy\n",
    "\n",
    "def test(model, test_data, loss_function, training = False):\n",
    "    # Test over complete test data\n",
    "    \n",
    "    test_accuracy_aggregator = []\n",
    "    test_loss_aggregator = []\n",
    "\n",
    "    for data in test_data:\n",
    "        pred = model(data)\n",
    "        label = tf.cast(data[2], dtype=tf.float32)\n",
    "        loss = loss_function(pred, label)\n",
    "        accuracy =  tf.reduce_mean(tf.cast(tf.equal(tf.greater(pred,0.5), tf.equal(label,1.0)), dtype=tf.float32))\n",
    "        test_loss_aggregator.append(np.mean(loss.numpy()))\n",
    "        test_accuracy_aggregator.append(accuracy)\n",
    "    \n",
    "    test_loss = np.mean(test_loss_aggregator)\n",
    "    test_accuracy = np.mean(test_accuracy_aggregator)\n",
    "      \n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QMSXHLtcbN9"
   },
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 960
    },
    "id": "-Qd4_xsscaw9",
    "outputId": "63da1f8e-3e34-4cde-ca6e-5bfab749bd83"
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "tf.keras.backend.clear_session()\n",
    "from IPython.display import clear_output\n",
    "\n",
    "### Hyperparameters\n",
    "num_epochs = 30\n",
    "learning_rate = 0.0001\n",
    "unit_num = 10\n",
    "running_average_factor = 0.95\n",
    "\n",
    "model_lstm = LSTM(unit_num)\n",
    "\n",
    "# Initialize the loss\n",
    "mean_squared_error = tf.keras.losses.MSE\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "# Initialize lists for later visualization.\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "# Testing model performance on train and test data before learning\n",
    "# train_loss, train_accuracy = test(model_lstm, train_dataset, mean_squared_error)\n",
    "# train_losses.append(train_loss)\n",
    "# train_accuracies.append(train_accuracy)\n",
    "\n",
    "# test_loss, test_accuracy = test(model_lstm, test_dataset, mean_squared_error)\n",
    "# test_losses.append(test_loss)\n",
    "# test_accuracies.append(test_accuracy)\n",
    "\n",
    "# Train loop for num_epochs epochs.\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch: __ ' + str(epoch))\n",
    "\n",
    "    # Training\n",
    "    running_average_loss = 0\n",
    "    running_average_accuracy = 0\n",
    "    for data in train_dataset:\n",
    "        train_loss, train_accuracy = train_step(model_lstm, data, mean_squared_error, optimizer)\n",
    "        running_average_loss = np.mean((running_average_factor * running_average_loss  + (1 - running_average_factor) * train_loss).numpy())\n",
    "        running_average_accuracy = np.mean((running_average_factor * running_average_accuracy  + (1 - running_average_factor) * train_accuracy).numpy())\n",
    "        \n",
    "    train_losses.append(running_average_loss)\n",
    "    train_accuracies.append(running_average_accuracy)\n",
    "   \n",
    "\n",
    "    # Testing\n",
    "    test_loss, test_accuracy = test(model_lstm, test_dataset, mean_squared_error)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "    \n",
    "    # Plotting\n",
    "    clear_output(wait=True) \n",
    "    fontsize=24\n",
    "\n",
    "    fig, ax = plt.subplots(2,1,sharex=True,figsize=(15,15))\n",
    "\n",
    "    ax[0].set_xlim((0,num_epochs))\n",
    "    ax[0].plot(train_losses,label=\"Training\")\n",
    "    ax[0].plot(test_losses,label=\"Test\")\n",
    "    ax[0].set_ylabel(\"Loss\",fontsize=fontsize)\n",
    "    ax[0].set_title(\"Epoch: \" + str(epoch), fontweight=\"bold\")\n",
    "    ax[0].legend()\n",
    "\n",
    "    ax[1].plot(train_accuracies,label=\"Training\")\n",
    "    ax[1].plot(test_accuracies,label=\"Test\")\n",
    "    ax[1].set_ylabel(\"Accuracy\",fontsize=fontsize)\n",
    "    ax[1].set_xlabel(\"Epochs\",fontsize=fontsize)\n",
    "    ax[1].set_ylim([0, 1])\n",
    "    ax[1].axhline(0.95,xmax=num_epochs,c='g',ls='--')\n",
    "\n",
    "    # original_plot = fig.add_subplot(2,2,3)\n",
    "    # original_plot.imshow(original.squeeze(), cmap = \"inferno\")\n",
    "    # original_plot.set_title(\"Original (\" + label + \")\")\n",
    "    # original_plot.axis(\"off\")\n",
    "\n",
    "    # reconstructed_plot = fig.add_subplot(2,2,4)\n",
    "    # reconstructed_plot.imshow(reconstructed.squeeze(), cmap = \"inferno\")\n",
    "    # reconstructed_plot.set_title(\"Reconstructed\")\n",
    "    # reconstructed_plot.axis(\"off\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    print('Train Accuracy: ',running_average_accuracy)\n",
    "    print('Test Accuracy: ',test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
