{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "LwRw8O_2jEE_"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXLfDM3ljRbo"
   },
   "source": [
    "### Helper function for timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer():\n",
    "    \"\"\"\n",
    "    A small class to measure time during training.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._start_time = None\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Start a new timer\n",
    "        \"\"\"\n",
    "        if self._start_time is not None:\n",
    "            print(f\"Timer is running. Use .stop() to stop it\")\n",
    "            return None\n",
    "\n",
    "        self._start_time = time.perf_counter()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"\n",
    "        Stop the timer, and report the elapsed time\n",
    "        \"\"\"\n",
    "        if self._start_time is None:\n",
    "            print(f\"Timer is not running. Use .start() to start it\")\n",
    "            return 0\n",
    "    \n",
    "        elapsed_time = time.perf_counter() - self._start_time\n",
    "        self._start_time = None\n",
    "        return elapsed_time  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXLfDM3ljRbo"
   },
   "source": [
    "# 1. Task\n",
    "For every timestep, two query digits are given. The network decides which of these two digits are most commonly presented in the sequence until the current timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<tf.Tensor: shape=(79,), dtype=uint8, numpy=\n",
      "array([5, 2, 9, 3, 9, 4, 6, 8, 1, 6, 3, 4, 3, 5, 9, 7, 5, 6, 9, 8, 4, 2,\n",
      "       4, 8, 2, 4, 1, 2, 4, 3, 9, 3, 7, 8, 5, 4, 4, 5, 1, 6, 8, 8, 5, 9,\n",
      "       2, 1, 7, 3, 2, 2, 6, 1, 1, 9, 6, 8, 6, 5, 5, 5, 8, 4, 5, 8, 4, 6,\n",
      "       6, 1, 5, 6, 5, 4, 3, 7, 8, 4, 1, 1, 9], dtype=uint8)>, <tf.Tensor: shape=(2,), dtype=uint8, numpy=array([1, 7], dtype=uint8)>, <tf.Tensor: shape=(2,), dtype=uint8, numpy=array([9, 4], dtype=uint8)>)]\n"
     ]
    }
   ],
   "source": [
    "def data_pair_gen(max_len=100):\n",
    "    while True:\n",
    "        length = np.random.randint(1, max_len+1)\n",
    "        x = np.random.randint(1, 10, length, dtype=np.uint8)\n",
    "        context = np.random.randint(1, 10, 2, dtype=np.uint8)\n",
    "        y = np.array([np.count_nonzero(x == value) for value in context], dtype=np.uint8)\n",
    "        \n",
    "        yield tf.constant(x), tf.constant(context), tf.constant(y)\n",
    "        \n",
    "# for tf==2.4.0\n",
    "# tf.data.Dataset.from_generator(generator=data_pair_gen,\n",
    "#                                output_signature=(tf.TensorSpec(shape=(None,), dtype=tf.uint32),\n",
    "#                                                  tf.TensorSpec(shape=(2,), dtype=tf.uint32),\n",
    "#                                                  tf.TensorSpec(shape=(2,), dtype=tf.uint32))\n",
    "#                               )\n",
    "\n",
    "# for tf==2.2.0\n",
    "ds = tf.data.Dataset.from_generator(generator=data_pair_gen,\n",
    "                                    output_types=(tf.uint8, tf.uint8, tf.uint8),\n",
    "                                    output_shapes=((None,), (2,), (2,))\n",
    "                                    )\n",
    "\n",
    "print(list(ds.take(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DMQjhAu_3xxX"
   },
   "source": [
    "# 2. Model\n",
    "To unroll the network, consider appending the network multiple times next to each other and feeding input at different locations. First do it using for loops, than change to graph mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-yu7GPj305a"
   },
   "outputs": [],
   "source": [
    "class LSTM_cell (tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim = 1):\n",
    "        super(LSTM_cell, self).__init__()\n",
    "        self.h = hidden_dim  # dimension of cell state and hidden state\n",
    "        # TODO: init hidden_state and cell_state\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # forget gate\n",
    "        self.w_f = self.add_weight(shape=(self.h,  # dim (h, d+h) with d = input_shape\n",
    "                                          self.h + input_shape),\n",
    "                                   initializer=tf.random_normal_initializer(),\n",
    "                                   trainable=True)\n",
    "        self.b_f = self.add_weight(shape=(self.h,),  # (h,1)\n",
    "                                   # bias of forget gate is initially 1\n",
    "                                   initializer=tf.keras.initializers.Constant(value=1.0),\n",
    "                                   trainable=True)\n",
    "        # input gate\n",
    "        self.w_i = self.add_weight(shape=(self.h, self.h + input_shape),\n",
    "                                   initializer=tf.random_normal_initializer(),\n",
    "                                   trainable=True)\n",
    "        self.b_i = self.add_weight(shape=(self.h,),\n",
    "                                   initializer=tf.random_normal_initializer(),\n",
    "                                   trainable=True)\n",
    "        # candidate layer\n",
    "        self.w_c = self.add_weight(shape=(self.h, self.h + input_shape),\n",
    "                                   initializer=tf.random_normal_initializer(),\n",
    "                                   trainable=True)\n",
    "        self.b_c = self.add_weight(shape=(self.h,), \n",
    "                                   initializer=tf.random_normal_initializer(),\n",
    "                                   trainable=True)\n",
    "        # output gate\n",
    "        self.w_o = self.add_weight(shape=(self.h, self.h + input_shape),\n",
    "                                   initializer=tf.random_normal_initializer(),\n",
    "                                   trainable=True)\n",
    "        self.b_o = self.add_weight(shape=(self.h,), \n",
    "                                   initializer=tf.random_normal_initializer(),\n",
    "                                   trainable=True)\n",
    "        \n",
    "\n",
    "    def call(self, input, (hidden_state, cell_state)):\n",
    "        # [h_{t-1}, x_t] to get dim: (d+h,1) where 1 is a single time slice\n",
    "        # TODO: axis might be wrong?\n",
    "        concat_input = tf.keras.layers.Concatenate(axis=0)([hidden_state, input])\n",
    "        \n",
    "        # function to compute ouput of forget, input, output gates\n",
    "        # e.g. f_t = sigmoid( w_f @ [h_t-1, x_t] + b_f )\n",
    "        gate_output = lambda w,b: tf.keras.activations.sigmoid(tf.linalg.matmul(w, concat_input) + b)\n",
    "        \n",
    "        # forget gate \n",
    "        f_t = gate_output(self.w_f, self.b_f)\n",
    "        # input gate\n",
    "        i_t = gate_output(self.w_i, self.b_i)\n",
    "        # candidates for new cell states, use tanh instead of sigmoid\n",
    "        c_tilde_t = tf.linalg.matmul(self.w_c, concat_input) + self.b_c\n",
    "        c_tilde_t = tf.keras.activations.tanh(c_tilde_t)\n",
    "        # update cell states: C_t = f_t * C_t-1 + i_t * C_tilde_t\n",
    "        self.cell_state = tf.math.multiply(f_t, self.cell_state) + tf.math.multiply(i_t, c_tilde_t)\n",
    "        # output gate\n",
    "        o_t = gate_output(self.w_o, self.b_o)\n",
    "        # h_t = o_t * tanh(C_t)\n",
    "        self.hidden_state = tf.math.multiply(o_t, tf.keras.activations.tanh(self.cell_state))\n",
    "                                        \n",
    "        return self.hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3MKwCKhwKv25"
   },
   "outputs": [],
   "source": [
    "class LSTM_net (tf.keras.Model):\n",
    "    '''\n",
    "    Build a LSTM net with a single recurrent node\n",
    "    '''\n",
    "    def __init__(self, hidden_dim=1):\n",
    "        super(LSTM_net, self).__init__()\n",
    "        # readin layer dim is subject to change depending on data structure\n",
    "        self.readin = tf.keras.layers.Dense(100, activation='relu', input_shape=(3,))\n",
    "        self.recurrent = LSTM_cell(hidden_dim)\n",
    "        # logistic classification\n",
    "        self.readout = tf.keras.layers.Dense(1, actiation='sigmoid')\n",
    "\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.readin(x)\n",
    "        x = self.recurrent(x)\n",
    "        x = self.readout(x)\n",
    "        return x\n",
    "\n",
    "    # TODO: unroll the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rz1gGPGHK3AO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "W09.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
