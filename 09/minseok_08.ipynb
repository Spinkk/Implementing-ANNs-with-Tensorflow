{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W09.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwRw8O_2jEE_"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXLfDM3ljRbo"
      },
      "source": [
        "# 1. Task\n",
        "For every timestep, two query digits are given. The network decides which of these two digits are most commonly presented in the sequence until the current timestep."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "Tq9myMUYnKtc",
        "outputId": "ae962272-705a-4950-d264-5e15214e7fa0"
      },
      "source": [
        "def generate_data_pair():\n",
        "    max_length = 100  # upper limit on length of a sequence\n",
        "    length = np.random.randint(1, max_length+1)  # decide sequence length\n",
        "    \n",
        "    input = np.random.randint(1, 10, (3,length))  # 1 digit + 2 queries\n",
        "    \n",
        "    output = np.zeros((length))  # most frequent element until t timepoint\n",
        "    for t in range(length):\n",
        "        output[t] = np.bincount(sequence[:t]).argmax()\n",
        "\n",
        "    return (input, output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: what type the data should be? ((context,sequence), target digit)?\n",
        "# currently I am thinking that input should be 3-dimensional, i.e. 2 context + 1 digit \n",
        "# and output should be 1-dim logistic decision\n",
        "tf.data.Dataset.from_generator(generate_data_pair, output_signature=)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-f3b398cbf990>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# currently I am thinking that input should be 3-dimensional, i.e. 2 context + 1 digit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# and output should be 1-dim logistic decision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_data_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m                 instructions)\n\u001b[0;32m--> 538\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     doc = _add_deprecated_arg_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_generator\u001b[0;34m(generator, output_types, output_shapes, args, output_signature)\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0moutput_types\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m         raise TypeError(\"Either `output_signature` or `output_types` must \"\n\u001b[0m\u001b[1;32m    823\u001b[0m                         \"be specified\")\n\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Either `output_signature` or `output_types` must be specified"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMQjhAu_3xxX"
      },
      "source": [
        "# 2. Model\n",
        "To unroll the network, consider appending the network multiple times next to each other and feeding input at different locations. First do it using for loops, than change to graph mode."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-yu7GPj305a"
      },
      "source": [
        "class LSTM_cell (tf.keras.layers.Layer):\n",
        "    def __init__(self, hidden_dim = 1):\n",
        "        super(LSTM_cell, self).__init__()\n",
        "        self.h = hidden_dim  # dimension of cell state and hidden state\n",
        "        # TODO: init hidden_state and cell_state\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # forget gate\n",
        "        self.w_f = self.add_weight(shape=(self.h,  # dim (h, d+h) with d = input_shape\n",
        "                                          self.h + input_shape),\n",
        "                                   initializer=tf.random_normal_initializer(),\n",
        "                                   trainable=True)\n",
        "        self.b_f = self.add_weight(shape=(self.h,),  # (h,1)\n",
        "                                   # bias of forget gate is initially 1\n",
        "                                   initializer=tf.keras.initializers.Constant(value=1.0),\n",
        "                                   trainable=True)\n",
        "        # input gate\n",
        "        self.w_i = self.add_weight(shape=(self.h, self.h + input_shape),\n",
        "                                   initializer=tf.random_normal_initializer(),\n",
        "                                   trainable=True)\n",
        "        self.b_i = self.add_weight(shape=(self.h,),\n",
        "                                   initializer=tf.random_normal_initializer(),\n",
        "                                   trainable=True)\n",
        "        # candidate layer\n",
        "        self.w_c = self.add_weight(shape=(self.h, self.h + input_shape),\n",
        "                                   initializer=tf.random_normal_initializer(),\n",
        "                                   trainable=True)\n",
        "        self.b_c = self.add_weight(shape=(self.h,), \n",
        "                                   initializer=tf.random_normal_initializer(),\n",
        "                                   trainable=True)\n",
        "        # output gate\n",
        "        self.w_o = self.add_weight(shape=(self.h, self.h + input_shape),\n",
        "                                   initializer=tf.random_normal_initializer(),\n",
        "                                   trainable=True)\n",
        "        self.b_o = self.add_weight(shape=(self.h,), \n",
        "                                   initializer=tf.random_normal_initializer(),\n",
        "                                   trainable=True)\n",
        "        \n",
        "\n",
        "    def call(self, input, (hidden_state, cell_state)):\n",
        "        # [h_{t-1}, x_t] to get dim: (d+h,1) where 1 is a single time slice\n",
        "        # TODO: axis might be wrong?\n",
        "        concat_input = tf.keras.layers.Concatenate(axis=0)([hidden_state, input])\n",
        "        \n",
        "        # function to compute ouput of forget, input, output gates\n",
        "        # e.g. f_t = sigmoid( w_f @ [h_t-1, x_t] + b_f )\n",
        "        gate_output = lambda w,b: tf.keras.activations.sigmoid(\n",
        "            tf.linalg.matmul(w, concat_input) + b)\n",
        "        \n",
        "        # forget gate \n",
        "        f_t = gate_output(self.w_f, self.b_f)\n",
        "        # input gate\n",
        "        i_t = gate_output(self.w_i, self.b_i)\n",
        "        # candidates for new cell states, use tanh instead of sigmoid\n",
        "        c_tilde_t = tf.linalg.matmul(self.w_c, concat_input) + self.b_c\n",
        "        c_tilde_t = tf.keras.activations.tanh(c_tilde_t)\n",
        "        # update cell states: C_t = f_t * C_t-1 + i_t * C_tilde_t\n",
        "        self.cell_state = tf.math.multiply(f_t, self.cell_state) + tf.math.multiply(i_t, c_tilde_t)\n",
        "        # output gate\n",
        "        o_t = gate_output(self.w_o, self.b_o)\n",
        "        # h_t = o_t * tanh(C_t)\n",
        "        self.hidden_state = tf.math.multiply(o_t, tf.keras.activations.tanh(self.cell_state))\n",
        "                                        \n",
        "        return self.hidden_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MKwCKhwKv25"
      },
      "source": [
        "class LSTM_net (tf.keras.Model):\n",
        "    '''\n",
        "    Build a LSTM net with a single recurrent node\n",
        "    '''\n",
        "    def __init__(self, hidden_dim=1):\n",
        "        super(LSTM_net, self).__init__()\n",
        "        # readin layer dim is subject to change depending on data structure\n",
        "        self.readin = tf.keras.layers.Dense(100, activation='relu', input_shape=(3,))\n",
        "        self.recurrent = LSTM_cell(hidden_dim)\n",
        "        # logistic classification\n",
        "        self.readout = tf.keras.layers.Dense(1, actiation='sigmoid')\n",
        "\n",
        "    \n",
        "    def call(self, x):\n",
        "        x = self.readin(x)\n",
        "        x = self.recurrent(x)\n",
        "        x = self.readout(x)\n",
        "        return x\n",
        "\n",
        "    # TODO: unroll the network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz1gGPGHK3AO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}