{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer():\n",
    "    \"\"\"\n",
    "    A small class for making timings.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self._start_time = None\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        Start a new timer\n",
    "        \"\"\"\n",
    "        if self._start_time is not None:\n",
    "            raise TimerError(f\"Timer is running. Use .stop() to stop it\")\n",
    "\n",
    "        self._start_time = time.perf_counter()\n",
    "\n",
    "    def stop(self):\n",
    "        \"\"\"\n",
    "        Stop the timer, and report the elapsed time\n",
    "        \"\"\"\n",
    "        if self._start_time is None:\n",
    "            print(f\"Timer is not running. Use .start() to start it\")\n",
    "            return 0\n",
    "    \n",
    "        elapsed_time = time.perf_counter() - self._start_time\n",
    "        self._start_time = None\n",
    "        return elapsed_time  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def occurance(seq, context):\n",
    "    occ = np.zeros((len(seq), len(context)), dtype=np.uint8)\n",
    "    counter = np.zeros(len(context))\n",
    "    \n",
    "    for i, val in enumerate(seq):\n",
    "        for e, c in enumerate(context):\n",
    "            counter[e] += int(c==val)\n",
    "            occ[i, e] = counter[e]\n",
    "            \n",
    "    return occ\n",
    "\n",
    "def data_generator(time_len=20):\n",
    "    while True:\n",
    "        x = np.random.randint(0, 10, time_len)\n",
    "        context = np.repeat(np.random.randint(0, 10, (1,2)), time_len, axis=0)\n",
    "        occ = occurance(x, context[0])\n",
    "        label = np.array([occ[:, 0]>=occ[:, 1], occ[:, 1]>=occ[:, 0]], dtype=np.uint8).T\n",
    "\n",
    "        # output_shape (time_len, 30), (time_len,2)\n",
    "        yield tf.concat([tf.one_hot(context[:, 0], 10), tf.one_hot(x, 10), tf.one_hot(context[:, 1], 10)], axis=-1), tf.constant(label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "ds = tf.data.Dataset.from_generator(generator=data_generator,\n",
    "                                    output_types=(tf.float32, tf.uint8),\n",
    "                                    # (time_len, 30), (time_len,2)\n",
    "                                    output_shapes=((None,30), (None,2))\n",
    "                                    )\n",
    "ds = ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_cell(tf.keras.layers.Layer):\n",
    "    def __init__(self, hidden_dim, input_shape=100):\n",
    "        super(LSTM_cell, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # weight matrices and bias vector shapes        \n",
    "        output_shape = [self.hidden_dim, 1]\n",
    "        weight_shape = [self.hidden_dim, input_shape + self.hidden_dim]\n",
    "        \n",
    "        # forget gate\n",
    "        self.w_f = self.add_weight(shape=weight_shape,\n",
    "                                   initializer=tf.random_normal_initializer(),\n",
    "                                   trainable=True)\n",
    "        \n",
    "        self.b_f = self.add_weight(shape=output_shape,\n",
    "                                   # bias of forget gate is initially 1\n",
    "                                   initializer=tf.keras.initializers.Constant(value=1.0),\n",
    "                                   trainable=True)\n",
    "        # input gate\n",
    "        self.w_i = self.add_weight(shape=weight_shape,\n",
    "                                   initializer=tf.random_normal_initializer(),\n",
    "                                   trainable=True)\n",
    "        \n",
    "        self.b_i = self.add_weight(shape=output_shape,\n",
    "                                   initializer=tf.random_normal_initializer(),\n",
    "                                   trainable=True)\n",
    "        \n",
    "        # candidate layer\n",
    "        self.w_c = self.add_weight(shape=weight_shape,\n",
    "                                   initializer=tf.random_normal_initializer(),\n",
    "                                   trainable=True)\n",
    "        \n",
    "        self.b_c = self.add_weight(shape=output_shape, \n",
    "                                   initializer=tf.random_normal_initializer(),\n",
    "                                   trainable=True)\n",
    "        # output gate\n",
    "        self.w_o = self.add_weight(shape=weight_shape,\n",
    "                                   initializer=tf.random_normal_initializer(),\n",
    "                                   trainable=True)\n",
    "        \n",
    "        self.b_o = self.add_weight(shape=output_shape, \n",
    "                                   initializer=tf.random_normal_initializer(),\n",
    "                                   trainable=True)\n",
    "    \n",
    "    def call(self, x_t, h_t, C_t):\n",
    "        x_t = tf.expand_dims(x_t, axis=-1)\n",
    "       \n",
    "        # concatenate previous hidden_state and current input (shape is 20,1)\n",
    "        concat = tf.concat([h_t, x_t], axis=1)\n",
    "        \n",
    "        # calculate f_t (the forget gate)\n",
    "        f_t = tf.nn.sigmoid(tf.linalg.matmul(self.w_f, concat) + self.b_f)\n",
    "\n",
    "        # calculate the input gate i_t\n",
    "        i_t = tf.nn.sigmoid(tf.linalg.matmul(self.w_i, concat) + self.b_i)\n",
    "        \n",
    "        # calculate the Cell state candidates (C_hat)\n",
    "        C_hat_t = tf.nn.tanh(tf.linalg.matmul(self.w_c, concat) + self.b_c)\n",
    "                \n",
    "        # calculate the Cell state (C_t)\n",
    "        C_t = f_t * C_t + i_t * C_hat_t\n",
    "        \n",
    "        # calculate the output state (o_t)\n",
    "        o_t = tf.nn.sigmoid(tf.linalg.matmul(self.w_o, concat) + self.b_o)\n",
    "        \n",
    "        # calculate the new hidden state (h_t)\n",
    "        h_t = o_t * tf.nn.tanh(C_t)\n",
    "               \n",
    "        return h_t, C_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_net(tf.keras.Model):\n",
    "    '''\n",
    "    Build a LSTM net with a single recurrent node\n",
    "    '''\n",
    "    def __init__(self, hidden_dim=100, readin_units=100):\n",
    "        super(LSTM_net, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Dense(readin_units)\n",
    "        self.cell = LSTM_cell(hidden_dim=self.hidden_dim, input_shape=readin_units) \n",
    "        \n",
    "        self.out = tf.keras.layers.Dense(2, activation = \"sigmoid\")\n",
    "\n",
    "#         # define hidden and cell state inside LSTM net instead of LSTM Cell, pass as argument in call\n",
    "#         self.hidden_state = tf.Variable(tf.zeros((1, self.hidden_dim, 1)), trainable=False)\n",
    "#         self.cell_state = tf.Variable(tf.ones((1, self.hidden_dim, 1)), trainable = False)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x, batch_size):     \n",
    "        # reset hidden and cell state before each call\n",
    "        self.hidden_state = tf.zeros((batch_size, self.hidden_dim, 1))\n",
    "        self.cell_state = tf.ones((batch_size, self.hidden_dim, 1))\n",
    "\n",
    "        # TODO: implement tensorArray for output\n",
    "        \n",
    "        output = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True)\n",
    "        # output = []\n",
    "        \n",
    "        for i in tf.range(x.shape[-2]):\n",
    "            x_t = self.embedding(x[:,i,:])\n",
    "            self.hidden_state, self.cell_state = self.cell(x_t, self.hidden_state, self.cell_state)  \n",
    "            output.write(i, self.out(tf.reshape(self.hidden_state, (batch_size,1,self.hidden_dim))))\n",
    "            # output.append(self.out(tf.reshape(self.hidden_state, (batch_size,1,self.hidden_dim))))\n",
    "\n",
    "        return output.concat()\n",
    "        #return tf.concat(output, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def train_step(model, ds, batch_size, loss_function, optimizer, train_loss_metric, train_acc_metric):\n",
    "    '''\n",
    "    Training for one epoch.\n",
    "    '''\n",
    "    \n",
    "    for seq, target in ds.take(100):\n",
    "        # forward pass with GradientTape\n",
    "        with tf.GradientTape() as tape:\n",
    "            prediction = model(seq, batch_size)\n",
    "            loss = loss_function(target, prediction)\n",
    "\n",
    "        # backward pass via GradienTape (auto-gradient calc)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        # update metrics\n",
    "        train_loss_metric.update_state(loss)\n",
    "        train_acc_metric.update_state(target, prediction)\n",
    "\n",
    "# def eval_step(model, ds, loss_function, loss_metric, acc_metric):\n",
    "#     '''\n",
    "#     Evaluation Loop.\n",
    "#     '''\n",
    "#     for img, target in ds:\n",
    "#         # forward pass\n",
    "#         prediction = model(img, training=False)\n",
    "#         # update metrics\n",
    "#         loss = loss_function(target, prediction)\n",
    "#         loss_metric.update_state(loss)\n",
    "#         acc_metric.update_state(target, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "learning_rate = 0.0005\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "timer = Timer()\n",
    "\n",
    "model = LSTM_net(hidden_dim=100, \n",
    "                 readin_units=1000)\n",
    "\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "# prepare metrics\n",
    "train_acc_metric = tf.keras.metrics.CategoricalAccuracy('train_accuracy')\n",
    "train_loss_metric = tf.keras.metrics.Mean('train_loss')\n",
    "\n",
    "# Initialize lists for later visualization.\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "times = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] - Started training of the lstm_net.\n",
      "\n",
      "[EPOCH] ____________________0____________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shapes (64, 20, 2) and (0, 1, 2) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1528b9f5c811>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_metric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc_metric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Evaluating training metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-562fce01dd4c>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, ds, batch_size, loss_function, optimizer, train_loss_metric, train_acc_metric)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# backward pass via GradienTape (auto-gradient calc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/losses.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    141\u001b[0m         y_true, y_pred, sample_weight)\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m       \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m       return losses_utils.compute_weighted_loss(\n\u001b[1;32m    145\u001b[0m           losses, sample_weight, reduction=self._get_reduction())\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/losses.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    244\u001b[0m       y_pred, y_true = tf_losses_util.squeeze_or_expand_dimensions(\n\u001b[1;32m    245\u001b[0m           y_pred, y_true)\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/losses.py\u001b[0m in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(y_true, y_pred, from_logits, label_smoothing)\u001b[0m\n\u001b[1;32m   1525\u001b[0m   y_true = smart_cond.smart_cond(label_smoothing,\n\u001b[1;32m   1526\u001b[0m                                  _smooth_labels, lambda: y_true)\n\u001b[0;32m-> 1527\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_logits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mcategorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m   4559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4560\u001b[0m   \"\"\"\n\u001b[0;32m-> 4561\u001b[0;31m   \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4562\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mfrom_logits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4563\u001b[0m     return nn.softmax_cross_entropy_with_logits_v2(\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1115\u001b[0m     \"\"\"\n\u001b[1;32m   1116\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (64, 20, 2) and (0, 1, 2) are incompatible"
     ]
    }
   ],
   "source": [
    "print(f'[INFO] - Started training of the {model.name}.')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\n[EPOCH] ____________________{epoch}____________________')\n",
    "    \n",
    "    # training step with metrics update--------------------------------------------------------\n",
    "    timer.start()\n",
    "\n",
    "    train_step(model, ds, batch_size, loss_function, optimizer, train_loss_metric, train_acc_metric)\n",
    "\n",
    "    # Evaluating training metrics\n",
    "    train_loss = train_loss_metric.result()\n",
    "    train_acc = train_acc_metric.result()\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    train_acc_metric.reset_states()\n",
    "    train_loss_metric.reset_states()\n",
    "    \n",
    "    elapsed_time = timer.stop()\n",
    "    times.append(elapsed_time)\n",
    "    \n",
    "    print(f'[{epoch}] - Finished Epoch in {elapsed_time:0.2f} seconds - train_loss: {train_loss:0.4f}, train_acc: {train_acc:0.4f}')\n",
    "  \n",
    "    if epoch%3 == 0:\n",
    "        print(f'\\n[INFO] - Total time elapsed: {np.sum(times)/60:0.4f} min. Total time remaining: {(np.sum(times)/(epoch+1))*(epochs-epoch-1)/60:0.4f} min.')\n",
    "\n",
    "print(f'[INFO] - Total run time: {np.sum(times)/60:0.4f} min.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,7))\n",
    "\n",
    "x = np.arange(len(train_losses))\n",
    "\n",
    "# losses\n",
    "axes[0].plot(x, train_losses, label='train')\n",
    "# axes[0].plot(x, test_losses, label='test')\n",
    "axes[0].legend()\n",
    "axes[0].set(title='Losses', xlabel='Epoch', ylabel='loss')\n",
    "\n",
    "# accuracies\n",
    "axes[1].plot(x, train_accuracies, label='train')\n",
    "# axes[1].plot(x, test_accuracies, label='test')\n",
    "axes[1].legend()\n",
    "axes[1].set(title='Accuracies', xlabel='Epoch', ylabel='accuracy')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
