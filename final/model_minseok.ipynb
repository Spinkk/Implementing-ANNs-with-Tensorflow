{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final_project.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPO3VNiULBAJc3Pnqp30gIP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Spinkk/Implementing-ANNs-with-Tensorflow/blob/main/final/model_minseok.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqOowDStD4oS"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eDeeoKKFsrx"
      },
      "source": [
        "# Hyperparam"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TicBo2lFsaf"
      },
      "source": [
        "z_dim = 100  # latent dim z_t\n",
        "c_dim = 256  # dim of g_ar output c_t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRuwXidaKXwf"
      },
      "source": [
        "# Model\n",
        "- Encoder: convolutional\n",
        "- Autoregressive: GRU\n",
        "- transformation of context: linear"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K1bAT6zEBwo"
      },
      "source": [
        "class Encoder (tf.keras.layers.Layer):\n",
        "    '''\n",
        "    g_enc\n",
        "    '''\n",
        "\n",
        "    def __init__ (self, z_dim=100):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = [\n",
        "                tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear'),\n",
        "                tf.keras.layers.BatchNormalization(),\n",
        "                tf.keras.layers.LeakyReLU(),\n",
        "                tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear') ,\n",
        "                tf.keras.layers.BatchNormalization() ,\n",
        "                tf.keras.layers.LeakyReLU() ,\n",
        "                tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear') ,\n",
        "                tf.keras.layers.BatchNormalization() ,\n",
        "                tf.keras.layers.LeakyReLU() ,\n",
        "                tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='linear') ,\n",
        "                tf.keras.layers.BatchNormalization() ,\n",
        "                tf.keras.layers.LeakyReLU() ,\n",
        "                tf.keras.layers.Flatten() ,\n",
        "                tf.keras.layers.Dense(units=256, activation='linear') ,\n",
        "                tf.keras.layers.BatchNormalization() ,\n",
        "                tf.keras.layers.LeakyReLU() ,\n",
        "                tf.keras.layers.Dense(units=z_dim, activation='linear', name='encoder_embedding') ,\n",
        "                ]\n",
        "\n",
        "    def call (self, x, training):\n",
        "        for l in self.layers:\n",
        "            try:  # batch normalization \n",
        "                x = l(x, training)\n",
        "            except:\n",
        "                x = l(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Autoregressive (tf.keras.layers.Layer):\n",
        "    '''\n",
        "    g_ar\n",
        "    '''\n",
        "\n",
        "    def __init__ (self, c_dim=256):\n",
        "        super(Autoregressive, self).__init__()\n",
        "        # dim: [batch, timesteps, feature]\n",
        "        self.l = tf.keras.layers.GRU(c_dim, name='ar_context') # TODO: return_sequence is false? \n",
        "    \n",
        "    def call (self, z):\n",
        "        return self.l(z)\n",
        "\n",
        "\n",
        "class Predict_z (tf.keras.layers.Layer):\n",
        "    '''\n",
        "    transformation of c_t, currently linear (W_k) for all future timesteps\n",
        "    '''\n",
        "\n",
        "    def __init__ (self, z_dim=100, max_k = 12):\n",
        "        super(Predict_z, self).__init__()\n",
        "        self.layers = []\n",
        "        for k in range(max_k):\n",
        "            self.layers.append(tf.keras.layers.Dense(z_dim)) # W_k for all k\n",
        "\n",
        "    def call(self, c_t):\n",
        "        predictions = []  # TODO: use tensorarrays\n",
        "        for l in self.layers:\n",
        "            predictions.append(l(c_t))\n",
        "\n",
        "        return predictions # dim: [k, batch, z_dim]\n",
        "\n",
        "\n",
        "def compute_f (z, predictions):\n",
        "    '''\n",
        "    compute density ratio following eq(3) in the paper\n",
        "    '''\n",
        "\n",
        "    z = tf.expand_dims(z, axis=-2)  # dim: [k, batch, 1, z_dim]\n",
        "    z = tf.cast(z, dtype='float64')\n",
        "    pred = tf.expand_dims(predictions, axis=-1)  # dim: k, batch, z_dim, 1]\n",
        "    pred = tf.cast(pred, dtype='float64')\n",
        "    return tf.squeeze(tf.linalg.matmul(z, pred), # compute dot product preserving k and batch dim\n",
        "                      axis=[-2,-1])  # [k,batch]\n",
        "\n",
        "\n",
        "class CPC (tf.keras.models.Model):\n",
        "    '''\n",
        "    put everything together. Return f_k for every k\n",
        "    '''\n",
        "\n",
        "    def __init__ (self, current_t, max_k, z_dim=100, c_dim=256):\n",
        "        super(CPC, self).__init__()\n",
        "        self.current_t = current_t  # split between end of g_ar and predictions\n",
        "        g_enc = Encoder(z_dim=z_dim)\n",
        "        g_ar = Autoregressive(c_dim=c_dim)\n",
        "        p_z = Predict_z(z_dim=z_dim, max_k=max_k)\n",
        "\n",
        "    def call(self, x):  \n",
        "        # Compute encodings TODO: use tensorarray instead of np zeros  \n",
        "        z_ts = np.zeros((17,1,100))  # [t, batch, z_dim]\n",
        "        for t in range(x.shape[1]):\n",
        "            z_ts[t] = (g_enc(x[:,t]))\n",
        "        # compute context vector at current timepoint\n",
        "        c_t = g_ar(tf.transpose(z_ts[:current_t],  \n",
        "                        perm=[1,0,2]))  # [batch, c_dim]\n",
        "\n",
        "        # compute transformations of c_t for every future time step\n",
        "        predictions = p_z(c_t)  # [k, batch, z_dim]\n",
        "        # compute ratios for every future time step\n",
        "        return compute_f(z_ts[current_t:], predictions)  # [k, batch]"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8DUcP0ZWL4O",
        "outputId": "71c423fd-b154-49c9-e99f-b2fe18597937"
      },
      "source": [
        "data = np.random.rand(1,17,10,10,1) # one batch, 17 time slices, each image has 10x10 shape and 1 feature\n",
        "data = tf.constant(data)\n",
        "cpc = CPC(current_t=5, max_k=12)  # 5 time for g_ar, 12 predictions\n",
        "cpc(data)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(12, 1), dtype=float64, numpy=\n",
              "array([[0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.],\n",
              "       [0.]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga0L03pPelg-"
      },
      "source": [
        "class InfoNCE (tf.keras.losses.Loss):\n",
        "    def call(self, f_k, px='uniform'):\n",
        "        if px == 'uniform':\n",
        "            return - np.log()  # TODO: which one is then positive?\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}