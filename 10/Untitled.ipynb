{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ds, trans_dict=False, sentence_wise=True):\n",
    "    # make numpy string array from tfds    \n",
    "    tfds_to_numpy = lambda x: next(iter(x))['text'].numpy()\n",
    "    ds = tfds_to_numpy(ds).decode()                             \n",
    "    \n",
    "    ds_words = ds.lower().replace('\\n', ' ').translate({ord(\"'\"): None})\n",
    "    exclude = string.punctuation.translate({ord(\"'\"): None})\n",
    "    table = ds_words.maketrans(exclude, ' '*len(exclude))                   \n",
    "    ds_words = ds_words.translate(table).split()\n",
    "    \n",
    "    # create a list of words split into sentences\n",
    "    if sentence_wise: \n",
    "        ds = ds.lower().replace('\\n', ' ').translate({ord(\"'\"): None})\n",
    "        exclude = string.punctuation.translate({ord(\"'\"): None, ord('.'): None})\n",
    "        table = ds.maketrans(exclude, ' '*len(exclude))\n",
    "        ds = ' '.join(ds.translate(table).split()).split('.')\n",
    "        ds = [sentence.translate({ord(\".\"): None}).split() for sentence in ds]        \n",
    "    \n",
    "    # create a list of words concatenated\n",
    "    else:\n",
    "        ds = ds_words\n",
    "    \n",
    "    if trans_dict:\n",
    "        # creates two lookup tables, val->id and id->val\n",
    "        dict_to_id = {val: i for i, val in enumerate(sorted(set(ds_words)))}        \n",
    "        dict_to_val = {id_: val for val, id_ in dict_to_id.items()}\n",
    "        \n",
    "        return ds, dict_to_id, dict_to_val\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['first', 'citizen', 'before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak'], ['all', 'speak', 'speak'], ['first', 'citizen', 'you', 'are', 'all', 'resolved', 'rather', 'to', 'die', 'than', 'to', 'famish', 'all', 'resolved'], ['resolved'], ['first', 'citizen', 'first', 'you', 'know', 'caius', 'marcius', 'is', 'chief', 'enemy', 'to', 'the', 'people']]\n"
     ]
    }
   ],
   "source": [
    "test_ds, train_ds = tfds.load(name='tiny_shakespeare',\n",
    "                        shuffle_files=False, \n",
    "                        split=['test', 'train'])\n",
    "\n",
    "\n",
    "test_ds, test_to_id, test_to_val = preprocess(test_ds, trans_dict=True)\n",
    "train_ds = preprocess(train_ds)\n",
    "\n",
    "print(train_ds[0:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ready', 'now')\n",
      "('am', 'now')\n",
      "('bring', 'to')\n",
      "('try', 'to')\n",
      "('with', 'to')\n",
      "('her', 'to')\n",
      "('clouds', 'thy')\n",
      "('to', 'thy')\n",
      "('strong', 'thy')\n",
      "('bidding', 'thy')\n",
      "('never', 'i')\n",
      "('had', 'i')\n",
      "('govern', 'to')\n"
     ]
    }
   ],
   "source": [
    "ds = test_ds\n",
    "\n",
    "def gen_word_embeddings():\n",
    "    while True:\n",
    "        np.random.shuffle(ds)      \n",
    "        \n",
    "        # for each sentence generate one target and make input, target pairs from leftover words within sentence\n",
    "        for sentence in ds:\n",
    "            target_id = np.random.randint(0, len(sentence))\n",
    "            target = sentence[target_id]\n",
    "            \n",
    "            context_window = sentence[target_id-2:target_id] + sentence[target_id+1:target_id+3]\n",
    "            np.random.shuffle(context_window)\n",
    "            \n",
    "            for word in context_window:\n",
    "                yield (word, target)\n",
    "                \n",
    "gen = gen_word_embeddings()\n",
    "for i in range(13):\n",
    "    print(next(gen))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
