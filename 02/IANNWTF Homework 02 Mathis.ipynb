{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to use\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-1*x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "def error_func(t, y_hat):\n",
    "    return 0.5 (t - y_hat)**2\n",
    "\n",
    "def error_func_prime(t,y_hat):\n",
    "    return -(t - y_hat)\n",
    "\n",
    "\n",
    "def accuracy_func(t, y_hat):\n",
    "    if ((t==1) and (y_hat>=0.5)) or ((t==0) and (y_hat<0.5)):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \n",
    "    def __init__(self, input_units, act_func= sigmoid):\n",
    "        # set number of inputs\n",
    "        self.input_units = input_units\n",
    "        \n",
    "        # initialize weighted connections from inputs to the Perceptron\n",
    "        self.weights = np.random.randn(input_units +1) # +1 for bias weights\n",
    "        \n",
    "        # set learning rate to be 0.01\n",
    "        self.lr = 0.01\n",
    "        \n",
    "        # initialize activation function variable\n",
    "        self.act_func = act_func\n",
    "        \n",
    "        # to keep track of this Perceptron's drive and activation, initialize variables for drive\n",
    "        # and the inputs it receives.\n",
    "        self.drive = 0\n",
    "        self.inputs = 0\n",
    "        self.activation = 0\n",
    "        \n",
    "        # initialize the error signal for the perceptron\n",
    "        self.delta = None\n",
    "        \n",
    "    def forward_step(self, inputs):\n",
    "        \"\"\"\n",
    "        Performs a forward step for the perceptron.\n",
    "        \"\"\"\n",
    "        \n",
    "        # insert a value of 1 as the first entry in the perceptrons inputs \n",
    "        #(activations from previous layer or the MLPs input) for the bias \n",
    "        self.inputs = np.insert(inputs,0,1)\n",
    "        \n",
    "        # calculate the weighted sum of inputs weighted by their weights\n",
    "        self.drive =  self.weights @ self.inputs\n",
    "        \n",
    "        # return the activation for the Perceptron given the inputs and weights(including bias)\n",
    "        self.activation = self.act_func(self.drive)\n",
    "        return self.activation\n",
    "    \n",
    "    def update(self, delta):\n",
    "        \"\"\"\n",
    "        This will allow to update weights associated with this perceptron given that we have it's delta.\n",
    "        \"\"\"\n",
    "        \n",
    "        # compute gradients of the weights to this perceptron (including bias) by multiplying it's error signal with\n",
    "        # the it's unweighted inputs (including a 1 at index 0 for the bias weights)\n",
    "        gradient_weights = delta * self.inputs\n",
    "        \n",
    "        # update the weights (including bias) by subtracting learning rate * gradients \n",
    "        self.weights -= self.lr * gradient_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    input dimensions, hidden dimensions and output dimensions given as lists in distinct arguments\n",
    "    \"\"\"\n",
    "    def __init__(self, total_dim):\n",
    "        \n",
    "        hidden_dim = total_dim[1:-1]\n",
    "        input_dim = total_dim[0]\n",
    "        output_dim = total_dim[-1]\n",
    "        print(total_dim)\n",
    "        \n",
    "        # create a nested list of Perceptrons where each perceptron has the input_units set to \n",
    "        # the previous number of units (including input units at n=0)\n",
    "        self.hidden_layers = [\n",
    "                [Perceptron(input_units = total_dim[n]) for _ in range(layer_units)] \n",
    "                 for n, layer_units in enumerate(hidden_dim)\n",
    "        ]\n",
    "        \n",
    "        # initialize perceptrons for the output units (in case we have multiple outputs)\n",
    "        self.output_units = [Perceptron(input_units = hidden_dim[-1]) for _ in range(output_dim)]\n",
    "        \n",
    "        # initialize MLP output to 0 and initialize an empty list for individual layer's perceptron's activations\n",
    "        self.output = [0 for _ in range(output_dim)]\n",
    "        self.layer_activations = []\n",
    "        \n",
    "    \n",
    "    def forward_step(self, inputs):\n",
    "        \"\"\"\n",
    "        Perform a forward step given inputs\n",
    "        \"\"\"\n",
    "        # make sure inputs match the input_units for the first hidden layer perceptrons.\n",
    "        assert len(inputs) == len(self.hidden_layers[0][0].weights) - 1,'input dimension should match the initalisation'\n",
    "        \n",
    "        self.layer_activations = [inputs]\n",
    "        \n",
    "        # for each layer calculate the activation for all it's perceptrons. \n",
    "        # Then store these activations in a list and store this list in a list for all layer's activations.\n",
    "        for layer in self.hidden_layers:\n",
    "            activations = []\n",
    "            for unit in layer:\n",
    "                unit_activation = unit.forward_step(self.layer_activations[-1])\n",
    "                activations.append(unit_activation)\n",
    "            self.layer_activations.append(activations)\n",
    "        \n",
    "        # calculate output\n",
    "        self.output = [out_unit.forward_step(self.layer_activations[-1]) for out_unit in self.output_units]\n",
    "        \n",
    "    def backprop_step(self, targets):\n",
    "        \n",
    "        error_signals = []\n",
    "        # Start by calculating error signals for the output units.\n",
    "        \n",
    "        # For this we use the formula delta_i = -(target_i - output_i) * sigmoid_prime(drive of output unit_i)\n",
    "        # create a nested list for the deltas in each layer\n",
    "        error_signals = []\n",
    "        output_deltas = []\n",
    "        for i, (target, output) in enumerate(zip(targets,self.output)):\n",
    "            \n",
    "            # we want to reuse the activation values and not recompute the sigmoid\n",
    "            delta_i = -(target - output) * (sigmoid_prime(self.output_units[i].activation))\n",
    "            \n",
    "            #store delta in a variable of the perceptron unit\n",
    "            self.output_units[i].delta = delta_i\n",
    "            \n",
    "            # append error signal to output error signals list\n",
    "            output_deltas.append(delta_i)\n",
    "            \n",
    "            # perform weight update for output unit i using the error signal associated with this unit\n",
    "            self.output_units[i].update(delta_i)\n",
    "            \n",
    "        error_signals.append(output_deltas)\n",
    "        \n",
    "        # now compute the error signals for the hidden layers, \n",
    "        # to do this, we compute the sums for all units in a layer simultaneously and then multiply\n",
    "        # the corresponding sum_i for unit_i in layer l with the sigmoid derivative of unit_i's drive.\n",
    "        \n",
    "        for n, layer in enumerate(reversed(self.hidden_layers)):\n",
    "            # initialize a list of error signals for this layer\n",
    "            layer_deltas = []\n",
    "            sums_for_layer = np.zeros(len(layer))\n",
    "            \n",
    "            # since output units and hidden units are stored in independent variables, we have a\n",
    "            # slightly different procedure for n == 0.\n",
    "            \n",
    "            if n == 0:\n",
    "                for o_unit_k in self.output_units:\n",
    "                    sums_for_layer += o_unit_k.delta * np.array(o_unit_k.weights[1:])\n",
    "                \n",
    "                for i, sum_i in enumerate(sums_for_layer):\n",
    "                    # multiply the sum for unit i by the sigmoid derivative of it's drive to obtain it's delta\n",
    "                    delta = sum_i * sigmoid_prime(layer[i].activation)\n",
    "                    layer_deltas.append(delta)\n",
    "                    layer[i].delta = delta\n",
    "                error_signals.append(layer_deltas)\n",
    "                    \n",
    "            else:\n",
    "                for hidden_unit_k in reversed(self.hidden_layers)[n+1]:\n",
    "                    sums_for_layer += hidden_unit_k.delta * np.array(hidden_unit_k.weights[1:])\n",
    "                    \n",
    "                for i, sum_i in enumerate(sums_for_layer):\n",
    "                    delta = sum_i * sigmoid_prime(layer[i].activation)\n",
    "                    layer_deltas.append(delta)\n",
    "                    layer[i].delta = delta\n",
    "                \n",
    "                error_signals.append(layer_deltas)\n",
    "        \n",
    "        # once we have all the deltas computed without having updated weights in between \n",
    "        #(this would distort the process), we update the weights.\n",
    "        \n",
    "        # update unit using the delta we assigned to it\n",
    "        for unit in self.output_units:\n",
    "            unit.update(unit.delta)\n",
    "            \n",
    "        for layer in self.hidden_layers:\n",
    "            for unit in layer:\n",
    "                unit.update(unit.delta) \n",
    "        \n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "target_and = np.array([0,0,0,1])\n",
    "target_or = np.array([0,1,1,1])\n",
    "target_nand = np.array([1,1,1,0])\n",
    "target_xor = np.array([0,1,1,0])\n",
    "target_nor = np.array([1,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.82042274, -0.78460147,  0.43058687])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "or_mlp = MLP([2,4,1])\n",
    "or_mlp.hidden_layers[0][0].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MLP object at 0x000001A378DA9848>\n"
     ]
    }
   ],
   "source": [
    "print(or_mlp)\n",
    "for x_i,t_i in zip(x,target_or):\n",
    "    or_mlp.forward_step(x_i)\n",
    "    or_mlp.backprop_step([t_i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.8204904 , -0.78464652,  0.43052614])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "or_mlp.hidden_layers[0][0].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
